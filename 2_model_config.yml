LeNet:
  activation: relu
  optimizer: adam
  pooling: max
MLP:
  activation: relu
  dropout: 0.2
  optimizer: Adam
  units:
  - 512
  - 512
MobileNet:
  alpha: 0.35
  dense_units:
  - 256
  - 256
  optimizer: adam
  weights: imagenet
Model: MLP
ResNet50:
  dense_units:
  - 256
  - 256
  optimizer: adam
upload:
  upload_name_filter:
  - None
  upload_sparse: 1.0
  upload_strategy: no-compress
